require(dplyr)
require(stringr)
require(raster)
require(rgdal)
require(rgeos)
require(randomForest)


# Function paramaters (params may be autogenerated by the build_model function): 
## * tile name = the name of the tile to subset training dataset from
## * folder_of_tiles = the file path to the folder that contains the tiles
## * file_path_to_shape = the file path to the shapefile. 
### Notes: This function has three steps
#### 1. reads single tile, creates an NDVI layer and merges the ndvi layer into the tile
#### 2. read the shapefile and rasterizes it
#### 3. clips the tile to the shapefile, converts the resulting object to tabular data and removes the NAs that resulted from teh clip. 
#### 4. Write the data to a file to be read in at a later time.  

subset_training_data_from_tile<- function( tile_name, folder_of_tiles, file_path_to_shape){
  
  
  # read raster make ndvi layer and add it to the brick.
  raster<-brick(paste0(paste(folder_of_tiles, tile_name ,sep="/"), ".tif" ))
  ## Rename Bands for ease of use
  names(raster)<-c("b1", "b2", "b3", "b4") 
  ## create NDVI layer
  ndvi<-overlay(raster$b4, raster$b3, fun=function(x,y){(x-y)/(x+y)})
  ## Merge NDVI and the tile imagery
  cov<-addLayer(raster,ndvi) 
  ## Rename all columns with...
  names(cov)<-c("b1", "b2", "b3", "b4", "ndvi")
  
  # parse and read shapefile
  
  ## split file path based on "/"
  temp<-stringr::str_split(file_path_to_shape, "/")
  ## Put in split file path into Read OGR to read shapefile and assign to variable.
  temp2<-readOGR(dsn=paste(head(temp[[1]], -1) , collapse = "/"), layer=paste(tail(temp[[1]], 1)))
  ## convert the Class in shapefile to numeric
  temp2@data$code<-as.numeric(temp2@data$Class)
  ## crop the shapefile to the extent of the current tile
  ### Then pipe to rasterize function so the the shape is converted to a raster using the field code to populate fields...with the same projection and tile size as the tile (represented by just the ndvi layer here)
  shape<-crop(temp2, extent(cov))%>%
    rasterize(ndvi, field="code") 
  ## Rename the code to class ????? Not sure why I did this. 
  names(shape)<-"class"
  
  # Remove unneeded objects from memory to free up compute power
  rm(ndvi, temp, temp2, raster)
  
  # set up dataset to be exported 
  ## Crops the covariate to the shape extent.  This step may seem redundent from the step above, where we cropped the shapefile to the extent of the tile, but it is necessary to decrease the size of processed data as much as possible. 
  data_to_be_modeled<-crop(cov, extent(shape))%>%
    ## Clip the tile to the shapefile geometry
    mask(shape)%>%
    ## add the rasterized shapefile to the clipped tile and ndvi info
    addLayer(shape)%>%
    ## convert the clipped info to tabular data
    getValues()%>%
    ## convert to data.frame.  This was the most effecient form to remove NAs memory wise.  This step adds time though. 
    as.data.frame()%>%
    ## exclude nas
    filter(!is.na(class))
  
  ## Save resulting dataset to file to be read in the next function. 
  saveRDS(data_to_be_modeled, paste0("training_datasets/", tile_name, "training_data.rds"))
  
  ## Remove the rest of the objects to clear memory after write happens.  If you don't do this objects get built up in memory and will eventaully make the function fail. 
  rm(data_to_be_modeled)
}





  
## Run random forest on combined training dataset
# - reads the folder with tiles
# -- takes all file names and stores them in a list to be transfered to the `subset_training_data_from_tile` function which
# - reads the shapefile
# -- adds the shapefile to `subset_training_data_from_tile`
# - tile names are looped over and sent to `subset_training_data_from_tile` individually with the shapefile. 
# - all individually returned tile training datasets are combined together with rbind.fill
# - combined dataset, is run through random forests. 
build_model<-function(folder_of_tiles, file_path_to_shape){
  ##list all files in the folder of tiles
  t<-base::list.files(folder_of_tiles)%>%
    ## remove file extension to get unique names
    tools::file_path_sans_ext()%>%
    ## remove second file name if it exists (some .tif files include a .tif.xml file)
    tools::file_path_sans_ext()%>%
    ## subset the resulting list to just unique values, the result is a list of all the unique file names in the folder
    base::unique() 
  
  # Not sure why this is necessary, but the function didn't work without it.  I'm guessing you have to initialize inputs in the function?????
  t2<-folder_of_tiles 
  # Same as above
  t3<-file_path_to_shape
  
  ## for the list of file names loop through each one run the `subset_training_data_from_tile_function`, with the inputs of `foler_of_tiles` and 'file path to shape'
  lapply(t, subset_training_data_from_tile, t2, t3)
  
  ## read all of the file names that you created in the last loop. 
  t4<-base::list.files("training_datasets")
  
  ## read all of the datasets based on the t4 file names in the previous step
  t5<-lapply(t4, function(x)readRDS(paste0("training_datasets/", x)))
  ## merge all training datasets together
  t6 <- do.call(plyr::rbind.fill, t5)
  
  ## remove the t5 list
  rm(t5)
  
  ## run random forests
  fit<-randomForest(as.factor(class)~.,
                    data=t6,
                    importance=TRUE,
                    ntree=500, norm.votes = FALSE
  )
  ## remove the dataset *** Should write this to a file so we don't have to create it every time.......
  rm(t6)
  return(fit)
}


## Run the function `build_model` which incorporates the `subset_training_data_from_tile` function.

## Todo create step that writes the model to an ouput (maybe based on the date, with the time it took). 
set.seed(420)
start_time <- Sys.time()

test_model<-build_model("test_tiles", "test_shape/training_polygons12N_12102018")

end_time <- Sys.time()

end_time - start_time


